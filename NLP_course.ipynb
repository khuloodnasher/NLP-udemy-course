{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP course.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkgBANw2Fc1ONkA2ignqZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khuloodnasher/NLP-udemy-course/blob/main/NLP_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fl-LAmPI9pi"
      },
      "source": [
        "person='Khulood'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFjvO9-cQJrb",
        "outputId": "8298ce15-431c-4cb9-b356-5b00bd5f4930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# old python\n",
        "print ('my name is {}'.format(person))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my name is Khulood\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA05NBDsQbJs",
        "outputId": "68b70015-51d7-4c5f-d582-d2bdc04b368f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'my name is {person}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my name is Khulood\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nohOVNhSTWYS",
        "outputId": "31592fe5-f4ca-4f99-84e5-8261e41bc111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'{person}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Khulood\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQQzpkdUIQG"
      },
      "source": [
        "d={'a':123,'b':456}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5RTE18CUlie",
        "outputId": "8188f961-5ae6-4f95-d0fd-a1c15e317b27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"my number is {d ['b']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my number is 456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpOIDSMXUrlL"
      },
      "source": [
        "mylist=[0,1,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8jjinNfV-O3",
        "outputId": "1e7314c0-b3b5-47ed-ae59-c4c12ddf0bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"my number is {mylist[1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my number is 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56ioQ6M1WIco"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthktE3GNPYP"
      },
      "source": [
        "To be able to train words in neural network, we have to give our words unique number "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuDlBDfZhav7"
      },
      "source": [
        "how to represent word to feed to neural net work\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-S52YKMhttS"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksFSBFFgh88I",
        "outputId": "77f6193d-0e61-41fa-f86e-56cc6da9ebfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# put our words in a list\n",
        "sentences=['I love my dog','I love my cat','You look cool']\n",
        "\n",
        "mysentence=Tokenizer(num_words=100) # just in case I want to add more words later\n",
        "#fitting the texts\n",
        "mysentence.fit_on_texts(sentences)# this line is taking each word from the sentences and give it a specific number\n",
        "indixingwords=mysentence.word_index\n",
        "print(indixingwords)\n",
        "# one good thing about tokinizer it returns everything lower case as in I and other letters\n",
        "# as we noticed each word got a unique number\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5, 'you': 6, 'look': 7, 'cool': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJb8cypZkiTN",
        "outputId": "990b9210-82da-459d-eb18-68d03476f0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now how to represent sentences using squence\n",
        "sentences=['I love my dog','I love my cat','You look cool']\n",
        "\n",
        "\n",
        "\n",
        "sequences1=mysentence.texts_to_sequences(sentences)\n",
        "print(sequences1)\n",
        "\n",
        "# sequences give numeric value to the tokinized words but the spliting the sentences into lists"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4], [1, 2, 3, 5], [6, 7, 8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YleMi2qhU5ij",
        "outputId": "ac3badd8-49b8-4818-8cae-830f55354490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# represent sentences using squence\n",
        "sentences=[\n",
        "    'My name is Sara',\n",
        "    'My age is 20'\n",
        "]\n",
        "\n",
        "\n",
        "mysentence=Tokenizer(num_words=100)\n",
        "mysentence.fit_on_texts(sentences)\n",
        "sequences1=mysentence.texts_to_sequences(sentences)\n",
        "print(sequences1)\n",
        "\n",
        "# sequences give numeric value to the tokinized words but the spliting the sentences into lists"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 3, 2, 4], [1, 5, 2, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1F_BnvrBels",
        "outputId": "72d09055-a105-4b32-f6d3-aa18f7104395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now we will test our tokenized text\n",
        "test_data=[\n",
        "    'you are amazing',\n",
        "    'loves my cat'\n",
        "]\n",
        "\n",
        "sequences2=mysentence.texts_to_sequences(test_data)\n",
        "print(sequences2)\n",
        "# As we noticed that the  word 'you' in training data and has number 6,but 'are' and 'amazing' were not recognized\n",
        "# word 'my' has numeric 3 and cat has numeric value 5"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6], [3, 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO3OnrUVDqGN",
        "outputId": "78387d41-a571-4b5a-e35b-8cffd45381cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# to avoid any missing meaning in the sentence I can give all the missing words a number, this number is unique but for the words that are not in training data set\n",
        "\n",
        "\n",
        "\n",
        "sentences=['I love my dog','I love my cat','You look cool']\n",
        "\n",
        "mynewsentence=Tokenizer(num_words=100,oov_token=\"<oov>\")# when we give numbers for each word, we will give the unknown words in training a specific number#fitting the texts\n",
        "mynewsentence.fit_on_texts(sentences)# this line is taking each word from the sentences and give it a specific number\n",
        "#indixingwords=mysentence.word_index\n",
        "#print(indixingwords)\n",
        "newsequences=mynewsentence.texts_to_sequences(sentences)\n",
        "print(newsequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 5], [2, 3, 4, 6], [7, 8, 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk7Q4VM9EWqO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERY6hPxDH3mx",
        "outputId": "da39d9fa-c9a7-422b-d2c9-e2e282b647bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now lets run our test again\n",
        "test_data=[\n",
        "    'you are amazing',\n",
        "    'loves my cat'\n",
        "]\n",
        "\n",
        "sequencestest=mynewsentence.texts_to_sequences(test_data)\n",
        "print(sequencestest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 1, 1], [1, 4, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuP6uOc9ICUE",
        "outputId": "0ffec965-9cae-4ddc-9130-466711602bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences=[\n",
        "    'I Love My Dog',\n",
        "    'I Love My Cat',\n",
        "    'you look cool'\n",
        "]\n",
        " \n",
        "mysentence1=Tokenizer(num_words=100,oov_token=\"<oov>\")\n",
        "mysentence1.fit_on_texts(sentences)\n",
        "word_index=mysentence1.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequences1=mysentence1.texts_to_sequences(sentences)\n",
        "#print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<oov>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6, 'you': 7, 'look': 8, 'cool': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p76SurWsBrgG",
        "outputId": "2decea76-df72-401c-e0a3-32b2427c83f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(sequences1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 5], [2, 3, 4, 6], [7, 8, 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ddFPwmBz3u",
        "outputId": "3008aff5-0e7d-4387-e8f3-feebd82196c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_data=[\n",
        "    'you are amazing',\n",
        "    'loves my cat'\n",
        "]\n",
        "\n",
        "sequences2=mysentence1.texts_to_sequences(test_data)\n",
        "print(sequences2)\n",
        "# we noticed that the new words in testing  has numeric number of one to show that those words are not included in trining"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 1, 1], [1, 4, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEvcvZAGCAvI",
        "outputId": "600b1d65-75bb-46a0-8ede-77e421c9c69a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Next we want to make all sentences with same length, so we have now sentences with three words ,others two or four words\n",
        "# to do this we are going use padding , where padding is going to take the max length of my sentences and naturalize according to it\n",
        "\n",
        "# Sentences Have Length\n",
        "padded=pad_sequences(sequences1)\n",
        "print(padded)\n",
        "# we noticed here that the shortest sentence has got zero for the first place holder of the first word so it has the same max length , this is called pre-padding because zero is added at the beginning, we can add the zero at the end as follows: padding='post'\n",
        "#which gives the sentence with length less than max length zero at the end so it will have same length\n",
        "# this is very important in neural net work because when we feed these sentences to neural network they must have same length, by default is pre, which means adding zero at the begnining"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3 4 5]\n",
            " [2 3 4 6]\n",
            " [0 7 8 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ0yUZBmIVtm",
        "outputId": "7b2b454c-5541-40f9-b689-655ff8a9b73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# if we want to add zero at the end just use padding ='post' as follows:\n",
        "\n",
        "padded1=pad_sequences(sequences1,padding='post')\n",
        "print(padded1)\n",
        "# so zero at end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3 4 5]\n",
            " [2 3 4 6]\n",
            " [7 8 9 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDK4-dg9K1gI",
        "outputId": "b36c3c55-3645-4531-fccf-6dccc9017f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# another important thing lets say we don't know the max length of my sentence, so i can just assume long max length but we should make sure this assumed max length is longer than any of our sentences so we will not miss any word for example max length=7, so it will give zeros for any shorter  sentences that have less words than 7 words but all sentences will have same maax length as follows\n",
        "\n",
        "padded2=pad_sequences(sequences1,padding='post',maxlen=9)\n",
        "print(padded2)\n",
        "# so zero at end"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2 3 4 0 0 0 0 0]\n",
            " [1 2 3 5 0 0 0 0 0]\n",
            " [6 7 8 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCcICegsLyxx",
        "outputId": "9237706e-bec1-4a3c-9e8f-3c7dde500500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences=[\n",
        "    'My name is Sara',\n",
        "    'My age is 20'\n",
        "]\n",
        "\n",
        "tokenizier=Tokenizer(num_words=100,oov_token=\"<oov>\")\n",
        "tokenizier.fit_on_texts(sentences)\n",
        "word_index=tokenizier.word_index\n",
        "print(word_index)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<oov>': 1, 'my': 2, 'is': 3, 'name': 4, 'sara': 5, 'age': 6, '20': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQlIg3pS5hCr",
        "outputId": "2604ba81-db02-4a46-b451-8b87fbad90fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences=[\n",
        "    'My name is Sara',\n",
        "    'My age is 20'\n",
        "]\n",
        "\n",
        "tokenizier=Tokenizer(num_words=100,oov_token=\"<oov>\")\n",
        "tokenizier.fit_on_texts(sentences)\n",
        "word_index=tokenizier.word_index\n",
        "print(word_index)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<oov>': 1, 'my': 2, 'is': 3, 'name': 4, 'sara': 5, 'age': 6, '20': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAzUFa285mPZ",
        "outputId": "db0c19a6-bb7f-4e61-db26-0698c01b4c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "full_text = \"hello world this is my problem can you solve it please?\"\n",
        "\n",
        "sequences = [[\"hello\", \"world\"], [\"my\", \"problem\"]]\n",
        "\n",
        "joined_sequences = [\" \".join(sequence) for sequence in sequences]\n",
        "\n",
        "def find_location(message, seq):\n",
        "    if seq in message:\n",
        "        return message.find(seq)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "output_dict = {}\n",
        "\n",
        "for sequence in joined_sequences:\n",
        "    start_index = find_location(full_text, sequence)\n",
        "    if start_index > -1:\n",
        "        output_dict[sequence] = [start_index, start_index+len(sequence)]\n",
        "\n",
        "print(output_dict)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hello world': [0, 11], 'my problem': [20, 30]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt1oQbgZ-kC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}